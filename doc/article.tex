\documentclass[11 pt, twocolumn]{article}

\usepackage{hyperref}
\usepackage{titling}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{color}
\usepackage{enumitem}
\setlist[enumerate]{label*=\arabic*.}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\setlength{\droptitle}{-5em}
\setlength{\columnsep}{2em}

\title{\textbf{GPU Monitoring tool for Reinforcement Learning algorithms}\\\Large CS236605 - Deep Learning, Computer Science Department\\Technion, Israel Institute of technology}

\author{Ira Gorodovskaya, Alexey Tusov}
\date{Spring 2019}

\newcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcommand{\algspace}{\hspace{\algorithmicindent}}

\begin{document}
\maketitle
\section{Abstract}
Graphics Processing Units (GPUs) can significantly accelerate the training process for many deep learning models. Training deep learning models involves compute-intensive matrix multiplication and other operations that can take advantage of a GPU's massively parallel architecture.

However in practice, average GPU utilization for deep learning jobs tends to be surprisingly low, mainly because of common made algorithms and without customization for used hardware accelerators.

In this paper, we present a practical method for monitoring GPU metrics for the Arcade Learning Environment (ALE) and use it to benchmark GPU utilization among different Reinforcement Learning algorithms and try to find dependence between learning performances and GPU utilization.


\section{Introduction}
We started our project by benchmarking Evolution Strategies (ES) algorithms beside reinforcement learning (RL) algorithms, trying to understand what can cause difference in performances, main and the significant difference we pointed was massive parallel execution in ES algorithms meanwhile RL algorithms can be parallel in some cases but in many scales less parallel then ES. This cause us to think about the accelerators utilization in learning algorithms.


Researches in reinforcement learning (RL) have relied heavily on empirical evaluation, making experiment turnaround time a key limiting factor. Despite this critical bottleneck, many reference implementations do not fulfill
the potential of modern computers for throughput.
Efforts to parallelize and accelerate deep RL algorithms have been underway for several years. Recently, (Horgan et al., 2018\cite{horgan2018distributed}) showed that a distributed, prioritized replay buffer can support faster
learning while using hundreds of CPU cores for simulation and a single GPU for training. The same work used increased batch sizes, with a brief study of the effect of
learning rate.


Based on GPU's architecture we assume that serial RL algorithms may not utilize all available computing power provided by the GPU. And we want to understand if better GPU utilization may cause better learning performances.
In this work, we provide our implementation for GPU runtime monitoring, this tool could help to improve leverage by maximize performance of multiple CPUs and single GPU system (we made also a compatibility to support multi GPU monitoring), in purpose to gain significant improvement in efficiency, scale of hardware utilization and hence in learning speed.


\section{Methods}
\subsection{GPU architecture}
Execution in GPUs made by calling kernel methods from user space in CPU's level, each user process can call multiple GPU's kernels in synchronic modes and asynhronic modes, multiplying user processes means multiplying GPU's concurrent executed kernels, each kernel executed in at least one Thread Block (TB) and all TBs executed in one Streaming Multiprocessor (SM), each 32 threads in one TB grouped in Wrap and executed in parallel command level context. GPU's architecture supports multiple number of SMs and they all shares same global GPU memory, user's code in CPU level copies memory from CPU's global memory to GPU's global memory.
Data copy could be done by DMA transfer, which requires pinned memory in CPU's global memory and CPU's low memory availability in large parallel executions could be a bottle neck for such mechanism. In addition, data may be transferred using unified memory, which requires additional page management and may cause and additional overhead.
\begin{figure}
\centering
\includegraphics[width=0.93\columnwidth]{gpu_arch.png}
\caption{GPU architecture}
\vspace*{2pt}
\end{figure}

\subsection{RL algorithms}
Provide brief introduction
\subsection{Non parallel RL algorithms}
Provide brief introduction for DQN, figures could be great
\subsection{Parallel RL algorithms}
Provide brief introduction for A2C, figures could be great
\subsection{Data for evaluation}
For evaluation we prepared monitoring implementation of A2C, DQN, DDQN, Dueling DQN and Rainbow algorithms for Atari games learning, due to technical limitations we choose to use DQN and A2C (maybe also rainbow?) algorithms to learn playing Atari game Pong.

\section{Implementation and Experiments}
\subsection{GPUMonitor and GPUMonitorWrapper}
[ADD LINK TO OUT GITHUB]


\textbf{GPUMonitor} is the main class for communication with GPUs and providing requested runtime information using \textbf{py3nvml} package, this class we used as reference from 
torchlearning repository [ADD REFERENCE].
Supported GPU's real time information obtained: 
\begin{enumerate}
\item Device name
\item Device memory
\begin{enumerate}
\item Total available memory [Mb]
\item Used memory [Mb]
\item Free memory [Mb]
\item List of used memory by different processes 
\end{enumerate}
\item Temperature [Celsius]
\item Power
\begin{enumerate}
\item Used power [W]
\item Power limitation [W]
\end{enumerate}
\item Fan speed [Percent]
\item Utilization
\begin{enumerate}
\item GPU utilization [Percent]
\item GPU memory utilization [Percent]
\end{enumerate}
\end{enumerate}


\textbf{GPUMonitorWrapper} is a wrapper class for RL environments, this class inherits from Wrapper class and could be added to model's wrappers hierarchy. It's supports logging directly to ".csv" log file every episode using \textbf{ResultWriter} class. \textbf{GPUMonitorWrapper} is our implementation and R\textbf{esultWriter} class implemented by use with reference to Monitor class from bench package.

\subsection{RL agents}
We used as reference DeepRL-Tutorials repository [ADD REFERENCE] for A2C, DQN, DDQN, Dueling DQN and Reinbow implementations for our agents implementation and experiments.

\subsection{Experiments}
We used DQN and A2C checking the following scenarios:
\begin{enumerate}
\item Comparing both models with default parameters
\item Checking influence CPUs number in A2C model
\item Checking influence of batch size in A2C model
\end{enumerate}
First of all we were interested to see if there is any confirmation for our assumption, better compute power utilization may lead to better learning performances, all our experiments done over PongNoFrameskip-v4 environment. Results are shown in Figure 2.
\begin{figure}
\centering
\includegraphics[width=0.93\columnwidth]{2c_32b.png}
\caption{Benchmarking different agents}
\vspace*{2pt}
\end{figure}



In our next experiment we focused on one model, we chose to focus on A2C model, before starting this experiment we assume that maybe the CPU is a bottle neck for increasing our compute power utilization, as we introduced in subsection 3.1 GPU architecture contains large amount of computing processors while the CPU have much less processors. Results are shown in Figure 3.
\begin{figure}
\centering
\includegraphics[width=0.93\columnwidth]{a2c_ncpus.png}
\caption{Benchmarking different number of CPUs}
\vspace*{2pt}
\end{figure}



In our last experiment we were interested if increasing the batch size may produce better performances. As we already seen, increasing utilization may increase learning performances, increasing batch size may also increase compute power utilization. For this experiment we used the same model of A2C agent and added additional execution with 512 batch size. Results are shown in figure 4.
\begin{figure}
\centering
\includegraphics[width=0.93\columnwidth]{a2c_ncpus_nbatch.png}
\caption{Benchmarking different number of CPUs and increasing batch size}
\vspace*{2pt}
\end{figure}

\section{Results}
As seen in figure 2, our assumption may be partly confirmed, better GPU utilization would lead to faster learning. But regarding GPU utilization we expected to see different result, intuitively more utilized execution should use more compute power but based on constant value of used memory in DQN agent we can assume that high value of GPU utilization could be explained with high over head of memory transfers which are blocking other calculations or even maybe it's performs different matrix calculations.


Our second experiment results we can see almost similar learning performances, in one hand we can see that less CPUs may learn faster in specific period but on the other hand we can see that more CPUs reached highest score earlier, similar to our assumption, and stays more stable in well learned model. Surprisingly, the implementation with 8 CPUs utilize less GPU compute power then similar one with 4 CPUs, it may be due to the difference in memory usage, while the implementation with 4 CPUs may have an overhead of data migration due to lack of memory the 8 CPUs implementation shouldn't.


From our last experiment we conclude that increasing batch size may utilize more GPU compute power in some calculations but it shows almost same results as other implementations, this may be a good result for example where trying to increase compute power utilization may not always be worthwhile.


In conclusion, we established our assumption of increasing compute power utilization may lead to better learning performances meanwhile we seen that considerations regarding this increments should be considered well avoiding the reversed results.


In addition we hope this tool maybe useful for researchers benchmarking their models for overview where and how it can be better utilized.


\section{References}

% I dont think it's relevant \ structured as they requested (https://vistalab-technion.github.io/cs236605/assignments/final-project)
% There may be relevant information, it may be copied to sections upper
%
%
%
%
%
%
%In a standard RL formulation as a Markov Decision Process, a learning agent aims to maximize the sum of discounted rewards experienced while interacting with an environment:
%$ R_t=\sum_{k=0}^\infty{\gamma^k r_{t+k}} $, where $r$ is the reward and $\gamma\leq 1$ the discount factor. The value of a state, $V(s_t)=\mathbb{E}\left[R_t|s_t\right]$, is defined as the expected return under a given policy.  The Q-value, $Q(s_t,a_t)=\mathbb{E}\left[R_t|s_t,a_t\right]$ is the same but first using action $a_t$ to advance.
%
%In policy gradient methods, the policy is directly parameterized as a distribution over actions, as $\pi(a|s;\theta)$. The \textbf{Advantage Actor-Critic} algorithm learns to estimate state values $V(s;\theta)$, and iteratively optimizes the policy on fresh environment experience using gradient steps as $\mathbb{E}\left[\nabla_\theta\log\pi(a_t|s_t;\theta)A_t\right]$, where $A(s,a)=Q(s,a)-V(s)$ is the advantage, estimated as $R_t - V(s_t)$.
%
%Q-value learning methods instead parameterize the Q-function $Q(s,a;\theta)$, which in \textbf{DQN} is regressed against an objective as: $\mathbb{E}[\left(y_i-Q(a_i|s_i;\theta)\right)^2]$, where $y_i$ is the data-estimated Q-value given by $y_i=r_i + \gamma \max_a Q(a|s_{i+1};\theta^-)$.  The target network $\theta^-$ is periodically copied from $\theta$.  Training data is selected randomly from a replay buffer of recent experiences, each to be used multiple times. The use of distributional learning was combined with five other enhancements under the name \textbf{Rainbow}: Double-DQN, Dueling Networks, Prioritized Replay, n-step learning, and NoisyNets.
%
%
%\section{Problem Description}
%
%\subsection{Main Objective}
%
%
%
%\subsection{Implementation}
%
%
%
%\section{Experiments}
%Common benchmarks for general RL algorithms are Atari games in the Arcade Learning Environment (ALE) for discrete control, and simulated robots using the MuJoCo physics engine in OpenAI
%Gym for continuous control.
%
%The Arcade Learning Environment (ALE) (Bellemare et al., 2013; Machado et al., 2017) is a framework composed of Atari 2600 games to develop and evaluate AI agents. OpenAI Gym, at https://gym.openai.com, is a toolkit for the development of RL algorithms, consisting of environments, e.g., Atari games and simulated robots, and a site for the comparison and reproduction of results.
%Try to draw your attention to
%
%\subsection{Setup}
%
%\subsection{Results}
%
%\section{Conclusion}
%GPUs are getting faster and faster but it doesn’t matter if the training code doesn’t completely use them. The good news is that for most people training machine learning models there is still a lot of simple things to do that will significantly improve efficiency.
%
%
%\bibliography{Refs}
%\bibliographystyle{ieeetr}
\end{document}