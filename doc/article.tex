\documentclass[11 pt, twocolumn]{article}

\usepackage{hyperref}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{color}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\setlength{\droptitle}{-5em}
\setlength{\columnsep}{2em}

\title{GPU Monitoring For RL Tasks}
\author{Ira Gorodovskaya, Alexey Tusov}
\date{Spring 2019}

\newcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcommand{\algspace}{\hspace{\algorithmicindent}}

\begin{document}
\maketitle
\section{Abstract}
Graphics Processing Units (GPUs) can significantly accelerate the training process for many deep learning models. Training models for tasks like image classification, video analysis, and natural language processing involves compute-intensive matrix multiplication and other operations that can take advantage of a GPU's massively parallel architecture.

However in practice, average GPU utilization for deep learning jobs tends to be surprisingly low. We can observe a quite similar picture with respect to average GPU memory usage.

In this paper, we present a practical method for monitoring GPU metrics for the Arcade Learning Environment (ALE) and use it to benchmark GPU utilization among different Reinforcement Learning algorithms and try to find dependence between learning performances and GPU utilization.


\section{Introduction}
Reinforcement learning (RL) is about an agent interacting with the environment, learning an optimal
policy, by trial and error, for sequential decision making problems, in a wide range of fields in natural
sciences, social sciences, and engineering.
A reinforcement learning agent observes states, executes actions, and receives rewards, with
major components of value function, policy and model. A RL problem may be formulated as a
prediction, control, or planning problem, and solution methods may be model-free or model-based,
and value-based or policy-based.

Research in deep reinforcement learning (RL) has relied heavily on empirical evaluation, making experiment
turnaround time a key limiting factor. Despite this critical bottleneck, many reference implementations do not fulfill
the potential of modern computers for throughput, unlike in supervised learning.


Efforts to parallelize and accelerate deep RL algorithms have been underway for several years. Recently, (Horgan et al., 2018\cite{horgan2018distributed}) showed that a distributed, prioritized replay buffer can support faster
learning while using hundreds of CPU cores for simulation and a single GPU for training. The same work used
increased batch sizes, with a brief study of the effect of
learning rate.


In this work, we study how to improve leverage by maximize performance of multiple CPUs and single GPU system, in purpose to gain significant improvement in efficiency, scale of hardware utilization and hence in learning speed.


\section{RL Algorithms Background}
In a standard RL formulation as a Markov Decision Process, a learning agent aims to maximize the sum of discounted rewards experienced while interacting with an environment:
$R_t=\sum_{k=0}^\infty{\gamma^k r_{t+k}}$, where $r$ is the reward and $\gamma\leq 1$ the discount factor. The value of a state, $V(s_t)=\mathbb{E}\left[R_t|s_t\right]$, is defined as the expected return under a given policy.  The Q-value, $Q(s_t,a_t)=\mathbb{E}\left[R_t|s_t,a_t\right]$ is the same but first using action $a_t$ to advance.

In policy gradient methods, the policy is directly parameterized as a distribution over actions, as $\pi(a|s;\theta)$. The \textbf{Advantage Actor-Critic} algorithm learns to estimate state values $V(s;\theta)$, and iteratively optimizes the policy on fresh environment experience using gradient steps as $\mathbb{E}\left[\nabla_\theta\log\pi(a_t|s_t;\theta)A_t\right]$, where $A(s,a)=Q(s,a)-V(s)$ is the advantage, estimated as $R_t - V(s_t)$.

Q-value learning methods instead parameterize the Q-function $Q(s,a;\theta)$, which in \textbf{DQN} is regressed against an objective as: $\mathbb{E}[\left(y_i-Q(a_i|s_i;\theta)\right)^2]$, where $y_i$ is the data-estimated Q-value given by $y_i=r_i + \gamma \max_a Q(a|s_{i+1};\theta^-)$.  The target network $\theta^-$ is periodically copied from $\theta$.  Training data is selected randomly from a replay buffer of recent experiences, each to be used multiple times. The use of distributional learning was combined with five other enhancements under the name \textbf{Rainbow}: Double-DQN, Dueling Networks, Prioritized Replay, n-step learning, and NoisyNets.


\section{Problem Description}

\subsection{Main Objective}



\subsection{Implementation}



\section{Experiments}
Common benchmarks for general RL algorithms are Atari games in the Arcade Learning Environment (ALE) for discrete control, and simulated robots using the MuJoCo physics engine in OpenAI
Gym for continuous control.

The Arcade Learning Environment (ALE) (Bellemare et al., 2013; Machado et al., 2017) is a framework composed of Atari 2600 games to develop and evaluate AI agents. OpenAI Gym, at https://gym.openai.com, is a toolkit for the development of RL algorithms, consisting of environments, e.g., Atari games and simulated robots, and a site for the comparison and reproduction of results.
Try to draw your attention to

\subsection{Setup}

\subsection{Results}

\section{Conclusion}
GPUs are getting faster and faster but it doesn’t matter if the training code doesn’t completely use them. The good news is that for most people training machine learning models there is still a lot of simple things to do that will significantly improve efficiency.


\bibliography{Refs}
\bibliographystyle{ieeetr}
\end{document}